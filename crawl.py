import os
import time
import requests
from bs4 import BeautifulSoup
import urllib.parse
import subprocess
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

# ğŸ§… Tor starten wenn nicht lÃ¤uft
def start_tor_if_needed():
    try:
        subprocess.run(["systemctl", "is-active", "--quiet", "tor"], check=True)
        print("âœ… Tor lÃ¤uft bereits.")
    except subprocess.CalledProcessError:
        print("ğŸš€ Starte Tor-Dienst...")
        subprocess.run(["sudo", "service", "tor", "start"], check=True)
        time.sleep(5)

# ğŸ” Neue IP holen & prÃ¼fen ob Google erreichbar ist
def restart_tor():
    print("ğŸ”„ Neue IP holen Ã¼ber Tor...")
    for _ in range(10):
        subprocess.run(["sudo", "service", "tor", "restart"])
        time.sleep(5)
        proxies = {
            'http': 'socks5h://127.0.0.1:9050',
            'https': 'socks5h://127.0.0.1:9050'
        }
        try:
            res = requests.get("https://www.google.com", proxies=proxies, timeout=10)
            if res.status_code == 200:
                print("ğŸŸ¢ Neue IP funktioniert.")
                return
        except:
            print("ğŸŸ¡ IP blockiert oder langsam... versuche andere Verbindung...")
    print("âŒ Konnte keine funktionierende IP finden. Breche ab.")
    exit()

# ğŸ“¸ Einzelbild speichern
def download_image(img_url, folder, count, proxies, downloaded):
    try:
        if img_url in downloaded or not img_url.startswith("http"):
            return False
        img_data = requests.get(img_url, proxies=proxies, timeout=10).content
        img_path = os.path.join(folder, f"image_{count}.jpg")  # Jeder Download bekommt eine einzigartige Nummer
        with open(img_path, "wb") as f:
            f.write(img_data)
        downloaded.add(img_url)
        print(f"âœ… Bild {count} gespeichert: {img_url}")
        return True
    except Exception as e:
        print(f"âš ï¸ Fehler beim Download von {img_url}: {e}")
        return False

# ğŸ” Bilder von Google ziehen
def download_images(search_query, folder, start_at, how_many, downloaded):
    base_url = "https://www.google.com/search?hl=de&tbm=isch&q={}&start={}"
    headers = {
        "User-Agent": "Mozilla/5.0"
    }
    proxies = {
        'http': 'socks5h://127.0.0.1:9050',
        'https': 'socks5h://127.0.0.1:9050'
    }

    if not os.path.exists(folder):
        os.makedirs(folder)

    count = len(downloaded)  # Starten mit der Anzahl der bereits heruntergeladenen Bilder
    with tqdm(total=how_many, desc="â¬‡ï¸ Downloadfortschritt") as pbar:
        for offset in range(start_at, start_at + 200, 20):
            url = base_url.format(urllib.parse.quote(search_query), offset)
            print(f"ğŸŒ Lade Seite {offset // 20 + 1} fÃ¼r '{search_query}'...")
            try:
                res = requests.get(url, headers=headers, proxies=proxies, timeout=10)
                if res.status_code != 200:
                    print(f"âš ï¸ Fehler beim Abrufen der Seite {offset}: Statuscode {res.status_code}")
                    break

                soup = BeautifulSoup(res.text, "html.parser")
                images = soup.find_all("img")

                img_urls = []
                for img in images:
                    img_url = img.get("src") or img.get("data-src")
                    if img_url and img_url.startswith("http") and img_url not in downloaded:
                        img_urls.append(img_url)

                if not img_urls:
                    print(f"ğŸ›‘ Keine neuen Bilder fÃ¼r '{search_query}' gefunden. Wechsel zum nÃ¤chsten Suchbegriff.")
                    return

                # ğŸ’¨ Paralleler Download
                with ThreadPoolExecutor(max_workers=5) as executor:  # Weniger Worker als vorher
                    futures = [
                        executor.submit(download_image, img_url, folder, count + i + 1, proxies, downloaded)
                        for i, img_url in enumerate(img_urls)
                    ]
                    for future in as_completed(futures):
                        if future.result():
                            count += 1
                            pbar.update(1)
                            if count >= how_many:
                                print(f"âœ… Zielanzahl von {how_many} Bildern erreicht fÃ¼r '{search_query}'!")
                                return
            except Exception as e:
                print(f"âŒ Fehler auf Seite {offset} fÃ¼r '{search_query}': {e}")
                continue

# ğŸš€ Hauptprogramm
if __name__ == "__main__":
    start_tor_if_needed()

    # Mehrere Suchbegriffe
    suchbegriffe = input("ğŸ” Gib Suchbegriffe ein, getrennt durch Kommas: ").split(",")
    ordner = input("ğŸ“ Zielordner: ")
    gesamt_anzahl = int(input("ğŸ“¸ Wie viele Bilder insgesamt? "))
    batch_size = 200  # Eine grÃ¶ÃŸere Anzahl, um auf mehrere Seiten zuzugreifen

    downloaded_urls = set()
    rounds = (gesamt_anzahl + batch_size - 1) // batch_size

    for suchbegriff in suchbegriffe:
        suchbegriff = suchbegriff.strip()  # Entfernt fÃ¼hrende und folgende Leerzeichen
        print(f"\nâš™ï¸ Starte Suche fÃ¼r: {suchbegriff}")

        for runde in range(rounds):
            print(f"\nâš™ï¸ Starte Runde {runde + 1}/{rounds} fÃ¼r '{suchbegriff}'")
            restart_tor()

            start_index = runde * 200
            images_left = gesamt_anzahl - len(downloaded_urls)
            to_download = min(batch_size, images_left)

            before = len(downloaded_urls)
            download_images(suchbegriff, ordner, start_index, to_download, downloaded_urls)

            if len(downloaded_urls) == before:
                print(f"â—ï¸ Keine neuen Bilder fÃ¼r '{suchbegriff}' gefunden. Wechsel zum nÃ¤chsten Suchbegriff.")
                break

    print(f"\nâœ… Fertig! Insgesamt {len(downloaded_urls)} Bilder gespeichert in '{ordner}'")

